{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key-Values RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/11 16:56:52 WARN Utils: Your hostname, 6miniui-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.200.112 instead (on interface en0)\n",
      "21/12/11 16:56:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/11 16:56:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"category-review-average\") # 로컬 환경과 앱 이름 지정\n",
    "sc = SparkContext(conf=conf) # 컨텍스트 초기화\n",
    "# 막간 에러: 자바 로컬 환경 변수를 변경해도 conda 가상환경 내 버전이 변경이 되지 않아 conda로 재인스톨하여 해결하였다.\n",
    "# !conda install -c anaconda openjdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "['id,item,category,reviews',\n",
       " '0,짜장면,중식,125',\n",
       " '1,짬뽕,중식,235',\n",
       " '2,김밥,분식,32',\n",
       " '3,떡볶이,분식,534',\n",
       " '4,라멘,일식,223',\n",
       " '5,돈가스,일식,52',\n",
       " '6,우동,일식,12',\n",
       " '7,쌀국수,아시안,312',\n",
       " '8,햄버거,패스트푸드,12',\n",
       " '9,치킨,패스트푸드,23']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"/Users/6mini/spark/res.csv\")\n",
    "lines.collect() # 간단한 액션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,짜장면,중식,125',\n",
       " '1,짬뽕,중식,235',\n",
       " '2,김밥,분식,32',\n",
       " '3,떡볶이,분식,534',\n",
       " '4,라멘,일식,223',\n",
       " '5,돈가스,일식,52',\n",
       " '6,우동,일식,12',\n",
       " '7,쌀국수,아시안,312',\n",
       " '8,햄버거,패스트푸드,12',\n",
       " '9,치킨,패스트푸드,23']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = lines.first() # 헤더 추출\n",
    "filtered_lines = lines.filter(lambda row: row != header)\n",
    "filtered_lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('중식', 125),\n",
       " ('중식', 235),\n",
       " ('분식', 32),\n",
       " ('분식', 534),\n",
       " ('일식', 223),\n",
       " ('일식', 52),\n",
       " ('일식', 12),\n",
       " ('아시안', 312),\n",
       " ('패스트푸드', 12),\n",
       " ('패스트푸드', 23)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse(row): # 카테고리와 리뷰 수 만을 파싱하는 함수\n",
    "    fields = row.split(\",\")\n",
    "    category = fields[2]\n",
    "    reviews = int(fields[3])\n",
    "    return (category, reviews) # KV RDD를 위해 튜플 형태로 두가지 리턴\n",
    "\n",
    "category_reviews = filtered_lines.map(parse) # KV RDD 생성\n",
    "category_reviews.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('중식', (125, 1)),\n",
       " ('중식', (235, 1)),\n",
       " ('분식', (32, 1)),\n",
       " ('분식', (534, 1)),\n",
       " ('일식', (223, 1)),\n",
       " ('일식', (52, 1)),\n",
       " ('일식', (12, 1)),\n",
       " ('아시안', (312, 1)),\n",
       " ('패스트푸드', (12, 1)),\n",
       " ('패스트푸드', (23, 1))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_reviews_count = category_reviews.mapValues(lambda x: (x, 1)) # 각 카테고리마다 값 하나를 추가\n",
    "category_reviews_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/6mini/opt/anaconda3/envs/spark-flink/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('중식', (360, 2)),\n",
       " ('분식', (566, 2)),\n",
       " ('일식', (287, 3)),\n",
       " ('아시안', (312, 1)),\n",
       " ('패스트푸드', (35, 2))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced = category_reviews_count.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) # 카테고리의 수와 리뷰 수의 총 합\n",
    "reduced.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('중식', 180.0),\n",
       " ('분식', 283.0),\n",
       " ('일식', 95.66666666666667),\n",
       " ('아시안', 312.0),\n",
       " ('패스트푸드', 17.5)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averages = reduced.mapValues(lambda x: x[0] / x[1]) # 평균\n",
    "averages.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Transformatieons & Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/11 17:17:04 WARN Utils: Your hostname, 6miniui-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.200.112 instead (on interface en0)\n",
      "21/12/11 17:17:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/11 17:17:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.app.id', 'local-1639210625743'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.port', '64503'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'transformations_actions'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', '192.168.200.112'),\n",
       " ('spark.app.startTime', '1639210624537')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "\n",
    "# 스파크 컨텍스트 생성\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"transformations_actions\")\n",
    "sc = SparkContext(conf = conf)\n",
    "sc.getConf().getAll() # 설정 리스트 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sc.stop() # 스파크 컨텍스트 중단\n",
    "# sc = SparkContext(conf=conf) # 다시 생성하여야 사용 가능\n",
    "\n",
    "foods = sc.parallelize([\"짜장면\", \"마라탕\", \"짬뽕\", \"떡볶이\", \"쌀국수\", \"짬뽕\", \"짜장면\", \"짜장면\", \"짜장면\",  \"라면\", \"우동\", \"라면\"]) # 텍스트파일 함수와 같이 RDD를 생성\n",
    "foods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "['짜장면', '마라탕', '짬뽕', '떡볶이', '쌀국수', '짬뽕', '짜장면', '짜장면', '짜장면', '라면', '우동', '라면']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD 데이터 확인\n",
    "# 벨류를 모두 가져올 수 있는데 디버깅이나 개발 환경에서만 사용해야하고 실제 프로덕트 상황에서 지양해야한다.\n",
    "# 데이터를 모두 가져오기 때문에 너무 낭비가 되기 때문에 스파크를 쓰는 의미가 없어진다.\n",
    "foods.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'짜장면': 4,\n",
       "             '마라탕': 1,\n",
       "             '짬뽕': 2,\n",
       "             '떡볶이': 1,\n",
       "             '쌀국수': 1,\n",
       "             '라면': 2,\n",
       "             '우동': 1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foods.countByValue() # 벨류 값 계산하여 키 벨류 형태로 전시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['짜장면', '마라탕', '짬뽕']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foods.take(3) # 3개의 엘리먼트만 전시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'짜장면'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foods.first() # 첫 값 전시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foods.count() # 엘리먼트 갯수 전시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/6mini/opt/anaconda3/envs/spark-flink/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foods.distinct().count() # 엘리먼트 갯수 유니크하여 전시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreach: 요소를 하나하나 꺼내어 하나의 함수를 작용시키는데 쓰인다.\n",
    "foods.foreach(lambda x: print(x))\n",
    "# 실행시켜도 리턴값은 없지만, 워커 노드에서 실행이 되기 때문에 현재 드라이버 프로그램에서는 보이지 않는다.\n",
    "# RDD를 연산하거나 로그를 세이브할 때 유용하게 쓰인다.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3]).map(lambda x: x + 2).collect() # 새로운 RDD 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['그린 북',\n",
       " '매트릭스',\n",
       " '토이 스토리',\n",
       " '캐스트 어웨이',\n",
       " '포드 V 페라리',\n",
       " '보헤미안 랩소디',\n",
       " '빽 투 더 퓨처',\n",
       " '반지의 제왕',\n",
       " '죽은 시인의 사회']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = [\n",
    "    \"그린 북\",\n",
    "    \"매트릭스\",\n",
    "    \"토이 스토리\",\n",
    "    \"캐스트 어웨이\",\n",
    "    \"포드 V 페라리\",\n",
    "    \"보헤미안 랩소디\",\n",
    "    \"빽 투 더 퓨처\",\n",
    "    \"반지의 제왕\",\n",
    "    \"죽은 시인의 사회\"\n",
    "]\n",
    "\n",
    "movies_rdd = sc.parallelize(movies)\n",
    "movies_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['그린',\n",
       " '북',\n",
       " '매트릭스',\n",
       " '토이',\n",
       " '스토리',\n",
       " '캐스트',\n",
       " '어웨이',\n",
       " '포드',\n",
       " 'V',\n",
       " '페라리',\n",
       " '보헤미안',\n",
       " '랩소디',\n",
       " '빽',\n",
       " '투',\n",
       " '더',\n",
       " '퓨처',\n",
       " '반지의',\n",
       " '제왕',\n",
       " '죽은',\n",
       " '시인의',\n",
       " '사회']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리스트를 펼쳐 보는 네로우 트랜스포메이션\n",
    "flat_movies = movies_rdd.flatMap(lambda x: x.split(\" \")) # 어떻게 나눌 지 지정\n",
    "flat_movies.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['그린',\n",
       " '북',\n",
       " '토이',\n",
       " '스토리',\n",
       " '캐스트',\n",
       " '어웨이',\n",
       " '포드',\n",
       " 'V',\n",
       " '페라리',\n",
       " '보헤미안',\n",
       " '랩소디',\n",
       " '빽',\n",
       " '투',\n",
       " '더',\n",
       " '퓨처',\n",
       " '반지의',\n",
       " '제왕',\n",
       " '죽은',\n",
       " '시인의',\n",
       " '사회']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_movies = flat_movies.filter(lambda x: x != \"매트릭스\") # 필터링\n",
    "filtered_movies.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/6mini/opt/anaconda3/envs/spark-flink/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/Users/6mini/opt/anaconda3/envs/spark-flink/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 집합과 관련된 트랜스포메이션\n",
    "num1 = sc.parallelize([1, 2, 3, 4])\n",
    "num2 = sc.parallelize([4, 5, 6, 7, 8, 9, 10])\n",
    "num1.intersection(num2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num1.union(num2).collect() # 모든 값이 합쳐진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num1.subtract(num2).collect() # 중복된 데이터가 사라진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 10]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_union = num1.union(num2)\n",
    "num_union.sample(True, .5, seed=6).collect() # True: 리샘플링 여부 / .5: 샘플링 할 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "짜 ['짜장면', '짜장면', '짜장면', '짜장면']\n",
      "마 ['마라탕']\n",
      "짬 ['짬뽕', '짬뽕']\n",
      "떡 ['떡볶이']\n",
      "쌀 ['쌀국수']\n",
      "라 ['라면', '라면']\n",
      "우 ['우동']\n",
      "치 ['치킨']\n",
      "돈 ['돈까스']\n",
      "회 ['회']\n",
      "햄 ['햄버거']\n",
      "피 ['피자']\n"
     ]
    }
   ],
   "source": [
    "# 와이드 트랜스포메이션\n",
    "foods = sc.parallelize([\"짜장면\", \"마라탕\", \"짬뽕\", \"떡볶이\", \"쌀국수\", \"짬뽕\", \"짜장면\", \"짜장면\", \"짜장면\",  \"라면\", \"우동\", \"라면\", \"치킨\", \"돈까스\", \"회\", \"햄버거\", \"피자\"])\n",
    "foods_group = foods.groupBy(lambda x: x[0]) # 그룹핑\n",
    "res = foods_group.collect()\n",
    "for (k, v) in res:\n",
    "    print(k, list(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "list(nums.groupBy(lambda x: x % 2).collect()[1][1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9eb6b6a293b30115e481ccb9f19506874aa8d9d4a5494c014ad03bfdeec02a3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('spark-flink': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
