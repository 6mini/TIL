{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04ff1429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/15 03:47:13 WARN Utils: Your hostname, 6miniui-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.200.112 instead (on interface en0)\n",
      "21/12/15 03:47:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/15 03:47:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/12/15 03:47:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/12/15 03:47:15 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "21/12/15 03:47:15 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "21/12/15 03:47:15 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "21/12/15 03:47:15 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "21/12/15 03:47:15 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"logistic-regression\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11db5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인에 포커스를 맞춘다.\n",
    "# 파이프라인은 데이터의 여러가지 처리 단계를 거칠 때 유용하게 쓰인다.\n",
    "# 대표적인 예가 텍스트 테이터로서, 토크나이징이나 글자의 수를 센다던지 원핫 인코딩을 하는 등의 여러가지 처리 방법이 있다.\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c2b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 데이터 생성\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4974718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글자를 스플릿하기 위한 토크나이저와 해싱 TF 생성\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8138b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=30, regParam=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "754acbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 생성\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3f76433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/15 03:54:06 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "21/12/15 03:54:06 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "21/12/15 03:54:07 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/12/15 03:54:07 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58495151",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91658da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa6756da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+----------+\n",
      "| id|              text|         probability|prediction|\n",
      "+---+------------------+--------------------+----------+\n",
      "|  4|       spark i j k|[0.63102699631690...|       0.0|\n",
      "|  5|             l m n|[0.98489377609773...|       0.0|\n",
      "|  6|spark hadoop spark|[0.13563147748816...|       1.0|\n",
      "|  7|     apache hadoop|[0.99563405823116...|       0.0|\n",
      "+---+------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select([\"id\", \"text\", \"probability\", \"prediction\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc6d951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
